#include <stdio.h>
#include <stdlib.h>

// Размер входной последовательности символов:
#define INPUT_SZ 8

// Отсчёт битов в числе начинается с нуля и справа.
// То есть в двоичном числе самый младший бит (нулевой бит) является крайним справа.
// Слева находится старший бит.
#define BIT_0 0x01  /* 00000001  В младший бит устанавливается 1. */
#define BIT_1 0x02  /* 00000010  В следующий бит (бит 1) устанавливается 1, предыдущий бит (бит 0) очищается. */
#define BIT_2 0x04  /* 00000100  В следующий бит (бит 2) устанавливается 1, младшие биты (бит 0 и 1) очищаются. */
#define BIT_3 0x08  /* 00001000  ... */
#define BIT_4 0x10  /* 00010000  ... */
#define BIT_5 0x20  /* 00100000  ... */
#define BIT_6 0x40  /* 01000000  ... */
#define BIT_7 0x80  /* 10000000  ... */

// Вывод последовательности символов:
void output(char* buffer, int n) {
    int i;
    for (i = 0; i < n; i++) {
        putc(buffer[i], stdout);
        printf(" %o \n", buffer[i]);
    }
}

// Вывод бинарного кода символа:
void printbin(int a) {
    int mask = 0x80;  /* 10000000 */
    while (mask > 0) {
        printf("%d", (a & mask) > 0);
        mask >>= 1;
    }
}

int main() {
    int i, odd;
    char buffer[INPUT_SZ];
    
    printf("Введите последовательность из %d символов: \n", INPUT_SZ);
    for (i = 0; i < INPUT_SZ; i++) {
        buffer[i] = getc(stdin);
    }
    
    //printf("Вывод исходной последовательности и ее восьмеричных кодов: \n");
    //output(buffer, INPUT_SZ);
    
    printf("Вывод исходной и преобразованной последовательностей и их восьмеричных и бинарных кодов: \n");
    printf("NO | CHAR OCT BIN 76543210 | CHAR OCT BIN 76543210 \n");
    
    // Замена в каждом из символов в их двоичном представлении:
    for (i = 0; i < INPUT_SZ; i++) {
        
        printf("%2d   ", i + 1);
        
        // Вывод символа из исходной последовательности и его восьмеричного кода.
        putc(buffer[i], stdout);
        printf("    %o     ", buffer[i]);
        
        printbin(buffer[i]);
        printf(" | ");
        
        odd = (i + 1) % 2 != 0;  // нечетность.
        if (odd) {  // для нечетных (по порядку) символов 3-й бит единицей:
            
            buffer[i] = buffer[i] | BIT_3;  // установить в 3-бит 1.
            
        } else {  // для четных символов 4-й бит нулем:
            
            buffer[i] = buffer[i] & (~BIT_4);  // установить в 4-бит 0.
            
        }
        
        // Вывод символа из преобразованной последовательности и его восьмеричного кода.
        putc(buffer[i], stdout);
        printf("    %o     ", buffer[i]);
        
        printbin(buffer[i]);
        printf("  // %d bit = %d \n", odd ? 3 : 4, odd);
    }
    
    //printf("Вывод преобразованной последовательности и ее восьмеричных кодов: \n");
    //output(buffer, INPUT_SZ);
    
    return 0;
}
